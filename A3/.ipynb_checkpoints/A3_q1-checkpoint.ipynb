{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16162968-85c1-45dc-89af-53ac0fd1470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47943e5-dc19-4277-9b37-84face84c714",
   "metadata": {},
   "source": [
    "# Q1: Face detection and association-based tracking [4.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0e591-cbff-4173-b087-161bd51de18a",
   "metadata": {},
   "source": [
    "## 1. [0.5 points] Data preparation."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0933e43b-d1f4-4d54-8869-3c1657c38341",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Download the video using youtube-dl\n",
    "VIDEO_URL=\"https://www.youtube.com/watch?v=bSMxl1V8FSg\"\n",
    "yt-dlp -f 'best[height<=480]' -o video.mp4 \"$VIDEO_URL\"\n",
    "# youtube-dl -f 'best[height<=480]' -o video.mp4 \"$VIDEO_URL\"\n",
    "\n",
    "# Extract frames using ffmpeg\n",
    "mkdir frames\n",
    "ffmpeg -i video.mp4 -vf fps=25 -t 30 frames/frame_%04d.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b177ff4-51ce-4258-8460-2fea1d7822c8",
   "metadata": {},
   "source": [
    "## 2. [1.5 points] Face detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfb992-4aab-4394-a4c7-2a5922e5ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f893c910-d664-47d2-82c7-764a40f5ddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minNeighbors = 50,num_stages=25 0.048168182373046875\n",
      "minNeighbors = 5,num_stages = 20 0.04134011268615723\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(f\"./frames/output_1.jpg\")\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "start = time.time()\n",
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=50, minSize=(30, 30))\n",
    "print(\"minNeighbors = 50,num_stages=25\",time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default-Copy1.xml')\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "print(\"minNeighbors = 5,num_stages = 20\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b24eb-38a5-4178-a7e0-7cd3c5b290a4",
   "metadata": {},
   "source": [
    "1. **Number of stages in the cascade:** The XML file contains information about the cascade structure. Each stage has a specific number of features to evaluate. Classifiers with more stages (typically for higher accuracy) take longer to process.\n",
    "\n",
    "1. **Minimum number of neighbors:** This parameter specifies how many neighboring detections are required to confirm a face. Evaluating more neighbors takes more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0824324-b926-438f-8150-e344d51b9d6c",
   "metadata": {},
   "source": [
    "## 3. [1 point] Face detection visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2def9755-a49d-44e9-a61a-3ce2114fcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_alt.xml')\n",
    "if not os.path.exists('output.mp4'):\n",
    "    video_path = 'video.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_path = 'output.mp4'\n",
    "    video_writer = cv2.VideoWriter(out_path, fourcc, fps,(frame_width,frame_height))\n",
    "    \n",
    "    for i in range(720):\n",
    "        img = cv2.imread(f\"./frames/output_{i+1}.jpg\")\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        video_writer.write(img)\n",
    "    \n",
    "        # cv2.imshow('Face Detection', img)\n",
    "        # key = cv2.waitKey(0) # goes to next image only when we press smtg\n",
    "        # # key = cv2.waitKey(1) # goes to next image automatically\n",
    "    \n",
    "        # if key == ord('q'):  # 'Esc' key\n",
    "        #     cv2.destroyAllWindows()\n",
    "        #     break\n",
    "    \n",
    "    # Release the capture\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b9f95-abe0-4efb-9df1-dc51498246d9",
   "metadata": {},
   "source": [
    "#### Face Detection Visualization Analysis\n",
    "\n",
    "**Video Link:** [Face Detection Video](https://drive.google.com/file/d/1z9JU6s31nj9wGWHrQglsO6SobZXHmJGO/view?usp=sharing)\n",
    "\n",
    "**Successful Detection Conditions**:\n",
    "  - Works well with clear, unobstructed views of faces.\n",
    "  - Effective in identifying faces without occlusion or motion blur.\n",
    "\n",
    "**Failure Scenarios**:\n",
    "  - Fails to detect faces from side or other views.\n",
    "  - Struggles in environments with complex or cluttered backgrounds, leading to false positives.\n",
    "  - Faces at a distance or in low-resolution frames may be missed entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc471d7-fc37-4472-9b47-3eb8bfde42f7",
   "metadata": {},
   "source": [
    "## 4. [1.5 point] Association-based tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab3b832-a7fe-48e2-aace-feb7070f1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tracker = {\n",
    "            \"bbox\" : [],\n",
    "            \"id\": [],\n",
    "            \"last_uniq_id\": -1\n",
    "        }\n",
    "    \n",
    "    def get_IOU(self,bbox1, bbox2):\n",
    "        \n",
    "        x1, y1, w1, h1 = bbox1\n",
    "        x2, y2, w2, h2 = bbox2\n",
    "    \n",
    "        x_left = max(x1, x2)\n",
    "        y_top = max(y1, y2)\n",
    "        x_right = min(x1 + w1, x2 + w2)\n",
    "        y_bottom = min(y1 + h1, y2 + h2)\n",
    "    \n",
    "        if x_right < x_left or y_bottom < y_top:\n",
    "            return 0.0\n",
    "    \n",
    "        intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "        # Calculate areas of the bounding boxes\n",
    "        bbox1_area = w1 * h1\n",
    "        bbox2_area = w2 * h2\n",
    "    \n",
    "        # Calculate union area\n",
    "        union_area = bbox1_area + bbox2_area - intersection_area\n",
    "    \n",
    "        # Calculate IoU\n",
    "        iou = intersection_area / union_area\n",
    "    \n",
    "        return iou\n",
    "\n",
    "    def update(self,cur_bboxes):\n",
    "        # print(cur_bboxes,self.tracker)\n",
    "        temp = self.tracker[\"bbox\"].copy()\n",
    "        # for id,bbox in enumerate(self.tracker[\"bbox\"]):\n",
    "        a=0\n",
    "        for id,bbox in enumerate(temp):\n",
    "            # print(id,bbox)\n",
    "            max_iou,max_index = -1,-1\n",
    "            for index,cur_bbox in enumerate(cur_bboxes):\n",
    "                iou = self.get_IOU(bbox,cur_bbox)\n",
    "                if (iou>max_iou):\n",
    "                    max_iou = iou\n",
    "                    max_index = index\n",
    "                \n",
    "            if max_iou < 0.5:\n",
    "                # print(\"no max_iou\")\n",
    "                self.tracker[\"bbox\"].pop(id-a)\n",
    "                self.tracker[\"id\"].pop(id-a)\n",
    "                a+=1\n",
    "            else:\n",
    "                # print(\"max_iou found\",max_index)\n",
    "                self.tracker[\"bbox\"][id-a] = cur_bboxes[max_index]\n",
    "                cur_bboxes = np.delete(cur_bboxes,max_index,axis=0)\n",
    "            # print(cur_bboxes,self.tracker)\n",
    "        while len(cur_bboxes)!=0:\n",
    "            self.tracker[\"last_uniq_id\"]+=1\n",
    "            self.tracker[\"bbox\"].append(cur_bboxes[0])\n",
    "            self.tracker[\"id\"].append(self.tracker[\"last_uniq_id\"])\n",
    "            cur_bboxes=np.delete(cur_bboxes,0,axis=0)\n",
    "        return self.tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fbaea8-caeb-4105-87cb-47c7c9e095a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('track_id.mp4'):\n",
    "    no_uniq_tracks = 0\n",
    "    track_vid = Tracker()\n",
    "    \n",
    "    # Video Writing Part\n",
    "    video_path = 'video.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_path = 'track_id.mp4'\n",
    "    video_writer = cv2.VideoWriter(out_path, fourcc, fps,(frame_width,frame_height))\n",
    "    \n",
    "    for i in range(720):\n",
    "        img = cv2.imread(f\"./frames/output_{i+1}.jpg\")\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        \n",
    "        # print(faces)\n",
    "        \n",
    "        tracker = track_vid.update(faces)\n",
    "        \n",
    "        # print(\"==============\",i)\n",
    "        \n",
    "        for index,bbox in enumerate(tracker[\"bbox\"]):\n",
    "            \n",
    "            # print(bbox,type(bbox))\n",
    "            x, y, w, h = bbox[0],bbox[1],bbox[2],bbox[3]\n",
    "            x_min,y_min = x,y\n",
    "            x_max,y_max = x+w,y+h\n",
    "            \n",
    "            cv2.rectangle(img, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n",
    "            # Put unique ID text\n",
    "            cv2.putText(img, str(tracker[\"id\"][index]), (int(x_min), int(y_min) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        cv2.imwrite(f\"./frames_track/img_{i}.jpg\",img)\n",
    "        video_writer.write(img)\n",
    "        no_uniq_tracks = tracker[\"last_uniq_id\"]\n",
    "\n",
    "    print(no_uniq_tracks)\n",
    "    # Release the capture\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08406a97-e0b2-4e1c-b42a-c7bb691a1370",
   "metadata": {},
   "source": [
    "No of unique Trackers = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c79ff6-86a8-4410-b37a-1f521abb98d6",
   "metadata": {},
   "source": [
    "#### Face Tracking Analysis\n",
    "\n",
    "**Video Link with Tracking IDs:** [Face Tracking Video](https://drive.google.com/file/d/1Jg7cPimln5NpuEXlVrfzlh3xDrlG9ZLK/view?usp=sharing)\n",
    "\n",
    "- **Distinct Track IDs for Individuals**:\n",
    "  - Different people generally receive unique track IDs, maintaining separation between individuals throughout the video.\n",
    "\n",
    "- **Challenges with Occlusion**:\n",
    "  - Occlusion may lead to multiple faces being associated with a single track ID (IoU > 0.5), especially when faces overlap or are close together.\n",
    "\n",
    "- **Failure Cases and Recommendations**:\n",
    "  - **0:00**: Motion blur causes a girl to be undetected in the next frame, resulting in a new track ID for her in subsequent frames.\n",
    "  - **0:03**: False positive detections from the background create unwanted new track IDs, affecting tracking accuracy.\n",
    "  - **0:05**: Small or low-resolution faces may not be detected, disrupting tracking continuity.\n",
    "\n",
    "##### Recommendations for Improvement\n",
    "\n",
    "- **Adjust Tracker Removal Criteria**:\n",
    "  - Extend the duration (e.g., to 5 frames) before removing a tracker to accommodate brief occlusions or missed detections.\n",
    "\n",
    "- **Explore Advanced Matching Techniques**:\n",
    "  - Implement alternative methods like template matching or color-based matching to improve face association in challenging scenarios such as occlusion or complex backgrounds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59854b7d-2f0e-420e-80b8-5238b97b5ea2",
   "metadata": {},
   "source": [
    "# Q2: YOLO Object Detection [5.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d06c54-a355-42f0-bc8c-cf53837d76a4",
   "metadata": {},
   "source": [
    "## 1. [0.5 point] Data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f3193-7867-4db5-8c50-9df35c797269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "# !kaggle datasets download -d haziqasajid5122/yolov8-finetuning-dataset-ducks\n",
    "# !unzip yolov8-finetuning-dataset-ducks.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602de04-b75d-49bc-bc52-998b4e10c111",
   "metadata": {},
   "source": [
    "**Training and Validation Data:**\n",
    "\n",
    "1. train/images: This folder contains the training images (e.g., *.jpg files).\n",
    "2. train/labels: This folder contains the annotation files (e.g., *.txt files) corresponding to the training images.\n",
    "3. valid/images: This folder contains the validation (or test) images.\n",
    "4. valid/labels: This folder contains the annotation files corresponding to the validation images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebceac08-29b4-480e-a4a0-1adea338ef0e",
   "metadata": {},
   "source": [
    "## 2. [1 point] Understanding YOLO object detector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2311a-f736-413a-9e75-62c64448d538",
   "metadata": {},
   "source": [
    "- Yolo is a single shot detector which predicts both bounding boxes and class probabilities in a single go unlike R-CNN series\n",
    "- In R-CNN series, it first proposes a region of interest using Region Proposal Network and now runs a classifier on each of these ROI to classify these proposed regions.\n",
    "- This two-stage process makes R-CNN series much slower than Yolo which requires only one pass through the network.\n",
    "- In R-CNN series, we have to train the region proposal network and the classification networks separately, while YOLO being end to end considers both localisation and classification tasks in the loss function jointly during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7b9d3-78fc-4398-bc4d-9784b4aea638",
   "metadata": {},
   "source": [
    "**Yolo Series**\n",
    "1. YOLOv1:\n",
    "- It divides the input images into a grid and predicts bounding boxes and class probabilities directly from the grid.But using this approach the model could not detect small objects.\n",
    "  \n",
    "2. YOLOv3:\n",
    "- It introduced a Feature Pyramid Network for multi-scale extraction.\n",
    "- It changed its backbone to Darknet-53 for feature extraction.\n",
    "- It predicts bounding box using logistic regression and it used anchor boxes to handle different aspect ratios\n",
    "\n",
    "3. YOLOv5:\n",
    "- It changed its backbone to a much more complex architechture EfficientDet\n",
    "- It introduced dynamic anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251eebc-6024-4acb-af91-94ce3f0e2a2c",
   "metadata": {},
   "source": [
    "## 3. [1 points] Hands on with ultralytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0d56a-e7da-4427-9213-44a36846ab84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc4676-c750-4b56-9370-2dab6b9474e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
    "\n",
    "# Print the total number of parameters\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Count the convolutional layers (excluding BatchNorm2d and other non-convolutional layers)\n",
    "num_conv_layers = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        num_conv_layers += 1\n",
    "\n",
    "print(f\"Number of Convolutional Layers: {num_conv_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47c451-c6bc-4ab7-90cf-d350d4fd9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m.yaml')  # build a new model from YAML\n",
    "\n",
    "# Print the total number of parameters\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Count the convolutional layers (excluding BatchNorm2d and other non-convolutional layers)\n",
    "num_conv_layers = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        num_conv_layers += 1\n",
    "\n",
    "print(f\"Number of Convolutional Layers: {num_conv_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf9b7e-2568-45a1-9c1e-15300896419f",
   "metadata": {},
   "source": [
    "## 4. [2 points] Training YOLO variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1febc98-9103-4ce6-a824-58fa836a6183",
   "metadata": {},
   "source": [
    "#### (i) Create two versions of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd90ced-eece-4d5d-8fb4-f8b7ba833f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    return ext.lower() in image_extensions\n",
    "    \n",
    "def get_image_paths(directory):\n",
    "    image_paths = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, file)):\n",
    "            if is_image_file(file):\n",
    "                image_paths.append(os.path.join(directory, file))    \n",
    "    return image_paths\n",
    "\n",
    "image_paths = get_image_paths(\"./datasets/archive/images/train\")\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cc146e7-65a7-4d60-8e23-f4af1981cf12",
   "metadata": {},
   "source": [
    "# Randomly select 100 images for train1\n",
    "train1_images = random.sample(image_paths, 100)\n",
    "\n",
    "train1_dir = \"./archive/images/train1\"\n",
    "train2_dir = \"./archive/images/train\"\n",
    "val_dir = \"./archive/images/val\"\n",
    "\n",
    "with open(\"./archive/config.yaml\", \"r\") as f:\n",
    "    config_content = f.read()\n",
    "\n",
    "# Modify data paths for train1\n",
    "with open(\"./archive/train1_config.yaml\", \"w\") as f:\n",
    "    modified_content = config_content.replace(\n",
    "        \"train: images/train\", f\"train: images/train1\"\n",
    "    )\n",
    "    f.write(modified_content)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f35941b6-63d8-4f67-add5-13b0c8c37fd8",
   "metadata": {},
   "source": [
    "for image_path in train1_images:\n",
    "    !cp \"{image_path}\" ./archive/images/train1\n",
    "\n",
    "image_paths = get_image_paths(\"./datasets/archive/images/train1\")\n",
    "for image_path in image_paths:\n",
    "    text = image_path.split(\"/\")[-1][:-4] +\".txt\"\n",
    "    text_path_source = \"./datasets/archive/labels/train/\"+text\n",
    "    text_path_dest = \"./datasets/archive/labels/train1/\"+text\n",
    "    !cp {text_path_source} {text_path_dest}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc100c2-8738-4167-b579-50e5851f469a",
   "metadata": {},
   "source": [
    "#### (ii) train three variants of the Yolo v8 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2e51a-becb-4680-9f3d-cbbf62d20655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cache_files(directory):\n",
    "    # Iterate through all the directories and files\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.cache'):\n",
    "                # Construct the full path to the file\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Attempt to remove the file\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Removed: {file_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error: {file_path} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b13ba-53de-4399-9c91-e9c4e6f8703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, config_path, epochs):\n",
    "\n",
    "    if \"train1\" in config_path:\n",
    "        project = \"yolov8_100img\"\n",
    "    else:\n",
    "        project = \"yolov8_400img\"\n",
    "        \n",
    "    pretrained=True\n",
    "    if \"yaml\" in model_name:\n",
    "        name = \"8n_scratch\"\n",
    "        pretrained=False\n",
    "    elif \"8n.pt\" in model_name:\n",
    "        name = \"8n_pretrained\"\n",
    "    elif \"8m.pt\" in model_name:\n",
    "        name = \"8m_pretrained\"\n",
    "\n",
    "\n",
    "    destination = f\"./{name}_{project[-6:]}.pt\"\n",
    "\n",
    "    if not os.path.exists(destination):\n",
    "        model = YOLO(model_name)\n",
    "        results = model.train(data=config_path, epochs=epochs, project=project, name=name, exist_ok=True,pretrained=pretrained,workers=8)\n",
    "        source = f\"./{project}/{name}/weights/best.pt\"\n",
    "        shutil.move(source, destination)\n",
    "        !rm -rf {project}\n",
    "        !rm -rf {\"wandb/\"}\n",
    "        remove_cache_files(\"./datasets/\")\n",
    "        return results\n",
    "    else:\n",
    "        print(\"already trained\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be24e6-fe68-4c48-99de-54458d59c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each variant\n",
    "variants = [\n",
    "    (\"yolov8n.yaml\", \"./datasets/archive/train1_config.yaml\", 20),\n",
    "    (\"yolov8n.pt\", \"./datasets/archive/train1_config.yaml\", 20),\n",
    "    (\"yolov8m.pt\", \"./datasets/archive/train1_config.yaml\", 20),\n",
    "    (\"yolov8n.yaml\", \"./datasets/archive/config.yaml\", 20),\n",
    "    (\"yolov8n.pt\", \"./datasets/archive/config.yaml\", 20),\n",
    "    (\"yolov8m.pt\", \"./datasets/archive/config.yaml\", 20),\n",
    "]\n",
    "\n",
    "results=[]\n",
    "for variant_name, config_path, epochs in variants:\n",
    "    print(f\"Training: {variant_name}\")\n",
    "    results.append(train_model(variant_name, config_path, epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f4ba5-9c45-494e-889b-99704938d010",
   "metadata": {},
   "source": [
    "#### (iii) Report and compare the results (AP50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df88b1c-6d93-4ecc-acf5-ed2ded3d5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weights_file(filename):\n",
    "    # image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp']\n",
    "    weights_extensions = [\".pt\"]\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    return ext.lower() in weights_extensions\n",
    "    \n",
    "def get_weights_paths(directory):\n",
    "    image_paths = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, file)):\n",
    "            if is_weights_file(file):\n",
    "                if(\"img\") in file:\n",
    "                    image_paths.append(os.path.join(directory, file))    \n",
    "    return image_paths\n",
    "\n",
    "weight_paths = get_weights_paths(\"./\")\n",
    "weight_paths.sort()\n",
    "print(weight_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43be91-747e-4b6a-a91c-9b3d09ade249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results.yaml\"):\n",
    "    results = {}\n",
    "    \n",
    "    for weight in weight_paths:\n",
    "        model = YOLO(weight)\n",
    "        \n",
    "        temp = \"train1\" if \"100\" in weight else \"train\"\n",
    "        \n",
    "        with open(\"./datasets/archive/config.yaml\", 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            config['val'] = f\"images/{temp}\"\n",
    "        \n",
    "        with open(\"./datasets/archive/val_config.yaml\", 'w') as new_file:\n",
    "            yaml.dump(config, new_file)\n",
    "    \n",
    "        remove_cache_files(\"./datasets/\")\n",
    "        val_map50 = model.val(data='./datasets/archive/config.yaml',workers=8).box.map50\n",
    "        remove_cache_files(\"./datasets/\")\n",
    "        train_map50 = model.val(data='./datasets/archive/val_config.yaml',workers=8).box.map50 \n",
    "        \n",
    "        results[weight] = {\n",
    "            'train_map50': str(train_map50),\n",
    "            'val_map50': str(val_map50)\n",
    "        }\n",
    "    \n",
    "    with open('results.yaml', 'w') as results_file:\n",
    "        yaml.dump(results, results_file, default_flow_style=False)\n",
    "    \n",
    "    print(\"Results saved to results.yaml file.\")\n",
    "else:\n",
    "    with open(\"./results.yaml\", 'r') as file:\n",
    "        results = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a0f30-95a5-4cf0-bee0-46d8027bd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926f3ca-0afd-41e9-ad9e-41ad3f3c932b",
   "metadata": {},
   "source": [
    "(a) Increasing dataset size generally improves model performance. For `yolov8m` (larger model), there's a slight decrease in validation mAP@50 (from `0.765` to `0.682`) when moving from 100 to 400 images, potentially due to overfitting. In contrast, `yolov8n` (smaller model) shows significant improvement (`0.620` to `0.745`) with more data, indicating better generalization.\n",
    "\n",
    "(b) The larger model (`yolov8m`) outperforms the smaller model (`yolov8n`) in most cases. `yolov8m` achieves higher validation mAP@50 (`0.765` for 100 images and `0.682` for 400 images) compared to `yolov8n` (`0.620` for 100 images and `0.745` for 400 images). This is likely due to `yolov8m`'s increased capacity to learn complex patterns, though it may suffer from overfitting with smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d2b25-bdef-4b9d-9b04-b4be73175410",
   "metadata": {},
   "source": [
    "#### (iv) Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43166f1f-f375-4628-a2d4-5c7e2d605e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"./8m_pretrained_100img.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fcb0e-9b0d-4d34-83dd-02d7da14899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = get_image_paths(\"./datasets/archive/images/val\")\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7535a-c2c1-4563-884e-2e6f05404384",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_images = random.sample(image_paths, 4)\n",
    "results = model.predict(train1_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e10db-b8b6-46b7-8173-87323ac817d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "for i,result in enumerate(results):\n",
    "    result.save(f\"result{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768f49c-f830-410b-a211-7db37df3eb2c",
   "metadata": {},
   "source": [
    "## 5. [1 point] Impact of augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c726d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random \n",
    "print(numpy.random.__file__) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e783480c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
