{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6866978-4fb3-4f00-86ff-0a14e9babb0c",
   "metadata": {},
   "source": [
    "# Q2: Contrastive Language-Image Pretraining [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cd58c-b4bb-474d-b0a2-80f86abd72cb",
   "metadata": {},
   "source": [
    "## 1. [1 point] Setup models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4763f0b9-1f74-4ad6-9250-b3ef49bed924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip \n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9a5e7-1570-4862-807b-cdade2f9c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "device1 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device2 = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ImageNet pretraining\n",
    "resnet50_imagenet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1).to(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc86216-3bf0-40c3-9369-bfb02f992ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI's CLIP\n",
    "clip_model, preprocess = clip.load(\"RN50\", device=device2)\n",
    "clip_image_encoder = clip_model.visual.to(device2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb369ef6-f402-4e4e-94e2-9df3158f7f42",
   "metadata": {},
   "source": [
    "The ImageNet-pretrained ResNet-50 has weights trained for image classification on the ImageNet dataset. The CLIP model's visual encoder, however, has been further trained on a large dataset of image-text pairs. This additional training allows the CLIP encoder to learn features that are not only good for classification but also for understanding the semantic relationship between images and text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86a6e3-cd95-48a5-b941-a342f6df3036",
   "metadata": {},
   "source": [
    "## 2. [1 point] Setup data. Understand the ImageNet challenge dataset\n",
    "\n",
    "The ImageNet challenge dataset uses the WordNet hierarchy for organizing its 1000 labels (also called synsets) associated with ILSVRC.\n",
    "\n",
    "**(i) Label Hierarchy (WordNet):**\n",
    "\n",
    "* WordNet is a lexical database that groups words into synonym sets (synsets) and connects them through defined relationships like \"is-a,\" \"part-of,\" etc.\n",
    "* ImageNet leverages this hierarchy by assigning each synset a unique label and using the WordNet structure to organize related concepts.\n",
    "\n",
    "**(ii) What is a Synset?**\n",
    "\n",
    "* A synset is a group of words or phrases that share the same meaning. In ImageNet, it represents a category of objects within the 1000 labels.\n",
    "* For example, the synset \"n01532825\" might correspond to the label \"golden retriever.\"\n",
    "\n",
    "**(iii) Problems with Grouping by Synsets for Visual Recognition?**\n",
    "\n",
    "* Yes, grouping objects based on synsets can lead to challenges for visual recognition tasks. Here's why:\n",
    "    * **Intra-class Variability:** Synsets can encompass objects with significant visual differences. A synset for \"dog\" might include images of golden retrievers, poodles, chihuahuas, etc. These breeds have distinct appearances that a model needs to learn.\n",
    "    * **Background Clutter:** Images within a synset might vary considerably due to background clutter. A \"chair\" synset could have images of chairs in different rooms, with different objects around them. The model needs to focus on the chair itself despite these variations.\n",
    "    * **Pose and Viewpoint:** Objects within a synset can appear in various poses and viewpoints. A \"car\" synset could have images of cars from the front, side, or back. The model needs to be robust to these pose variations.\n",
    "\n",
    "\n",
    "**(iv) 3 Types of Visual Differences in Images with the Same Synset:**\n",
    "\n",
    "1. **Object Appearance:** As mentioned earlier, objects within a synset can have significant visual differences in terms of breed, shape, size, color, or material.\n",
    "2. **Background Complexity:** Images can vary in background complexity, with objects appearing in cluttered environments, outdoors, or with other objects around them.\n",
    "3. **Pose and Viewpoint:** The pose and viewpoint of the object can differ significantly within a synset. Objects can be tilted, rotated, or partially occluded, requiring the model to recognize them from various perspectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af9634-0065-4f14-8cf8-101a5864850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Setup zero-shot CLIP\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_features(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image = Image.open(f).convert(\"RGB\")\n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image)\n",
    "    return image_features\n",
    "\n",
    "def compute_probability_scores(image_features):\n",
    "    text_features = []\n",
    "    with torch.no_grad():\n",
    "        for label in tqdm(IMAGENET2012_CLASSES.values()):\n",
    "            text = clip.tokenize([label]).to(device)\n",
    "            curr_features = clip_model.encode_text(text)\n",
    "            text_features.append(curr_features)\n",
    "    text_features = torch.cat(text_features, dim=0)\n",
    "    logits_per_image = torch.cosine_similarity(image_features, text_features, dim=-1)\n",
    "    probs = torch.softmax(logits_per_image, dim=-1)\n",
    "    return probs\n",
    "\n",
    "# Test CLIP on a few example images\n",
    "example_images = os.listdir('imagenet-sample-images')\n",
    "example_images = [example_image for example_image in example_images if example_image.endswith('.JPEG')]\n",
    "\n",
    "example_images = example_images[:5]\n",
    "\n",
    "classes_list = list(IMAGENET2012_CLASSES.values())\n",
    "for image_path in example_images:\n",
    "    image_features = get_image_features(os.path.join('imagenet-sample-images', image_path))\n",
    "    probs = compute_probability_scores(image_features)\n",
    "    top5_indices = torch.topk(probs, k=5, dim=-1).indices.tolist()\n",
    "    top5_categories = [classes_list[idx] for idx in top5_indices]\n",
    "    print(f\"Image: {image_path}\")\n",
    "    print(f\"Top 5 predicted categories: {top5_categories}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
